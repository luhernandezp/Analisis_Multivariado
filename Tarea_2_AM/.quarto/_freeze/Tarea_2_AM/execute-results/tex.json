{
  "hash": "be0dda49a0ca0cd8404b237a87f9cb4e",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlang: es\nformat:\n  pdf:\n    documentclass: article\n    include-in-header:\n      - file: preamble.tex\nexecute:\n  cache: true\n---\n\n\n\n\\pagenumbering{gobble}\n\\begin{titlepage}\n\\newcommand{\\HRule}{\\rule{\\linewidth}{0.5mm}} % Defines a new command for the horizontal lines\n\\center\n\n\\begin{minipage}{13.5cm}\n\\center\n\\includegraphics[width=3cm,height=4cm]{logo}\\\\[0.5cm] % Logo\n\n\\textsc{\\Large UNIVERSIDAD NACIONAL DE COLOMBIA \\\\[1.0cm]\n{\\large MAESTRÍA EN CIENCIAS ESTADÍSTICA\\\\[0.5cm]\nDepartamento de Estadística\\\\[0.2cm]\nFacultad de Ciencias}}\\\\[2cm]\n\n\\rule[1.7mm]{0.3cm}{0.5mm}\n\\hfill\n\\textsc{\\Large Análisis Multivariado de Datos}\n\\hfill\n\\rule[1.7mm]{0.3cm}{0.5mm}\n\\\\[0.2cm]\n\n\n\n\\rule{\\linewidth}{0.5mm}\\\\[1.5cm]\n\n{\\large \\textbf{Integrantes}:\\\\[0.3cm]\n\\begin{tabular}{cc}\nLuis David Hernández Pérez & C.C. 1193549963 \\\\\nDaniel Felipe Villa Rengifo & C.C. 1005087556 \\\\\n\\end{tabular}\n}\\\\[2.5cm]\n\n{\\large\nMedellín, Colombia \\\\\nSemestre 2024-02\n}\\\\[0.3cm]\n\n{\\large\nMedellín, Enero 31 de 2025\n}\n\\end{minipage}\n\\vfill\n\\end{titlepage}\n\\pagenumbering{arabic}\n\n\n\n\\tableofcontents\n\\newpage\n\n\n\nLos datos utilizados son los pertenecientes al equipo-07\n\n# Punto-01:\n\nConsidere la matriz de datos asignada, la cual corresponde a un conjunto de datos simulados de un vector $\\mathbf{x}$ normal 6-variado con parámetros dados por:\n\n$$\n\\boldsymbol{\\mu} = \n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\mu_4 \\\\\n\\mu_5 \\\\\n\\mu_6\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}, \\quad\n\\boldsymbol{\\Sigma} = \n\\begin{bmatrix}\n4 & 0 & 2 & 0 & 1 & 0 \\\\\n0 & 9 & 0 & 3 & 0 & 2 \\\\\n2 & 0 & 5 & 0 & 4 & 0 \\\\\n0 & 3 & 0 & 8 & 0 & 1 \\\\\n1 & 0 & 4 & 0 & 6 & 0 \\\\\n0 & 2 & 0 & 1 & 0 & 7\n\\end{bmatrix}\n$$\n\nParticione $\\mathbf{x}$ como sigue:\n\n$$\n\\mathbf{x} = \n\\begin{bmatrix}\n\\mathbf{x}^{(1)} \\\\\n\\mathbf{x}^{(2)}\n\\end{bmatrix},\n\\quad \\text{donde:} \\quad\n\\mathbf{x}^{(1)} = \n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{bmatrix}, \\quad\n\\mathbf{x}^{(2)} = \n\\begin{bmatrix}\nX_4 \\\\\nX_5 \\\\\nX_6\n\\end{bmatrix}\n$$\n\n\\begin{enumerate}[label=(\\alph*)]\n    \\item Realice una verificación de la normalidad: uni-variada, bi-variada y 3-variada de los datos asociados a $\\mathbf{x}^{(1)}$. \\\\\n    \n\\textbf{Nota:} Utilizar algunas de las herramientas vistas en clase sobre procesos de evaluación de la normalidad multivariada y/o herramientas que usted conozca para dichos procesos.\n    \n\\item ¿Cuáles son los estimadores de Máxima Verosimilitud de $\\boldsymbol{\\mu}^{(1)} = \\mathbb{E}[\\mathbf{x}^{(1)}]$ y de $\\boldsymbol{\\Sigma}_{11} = \\mathrm{Var}(\\mathbf{x}^{(1)})$?\n\n  \\item Considere la variable definida por $Y = \\mathbf{a}^T \\mathbf{x}^{(2)}$, con $\\mathbf{a} = \n    \\begin{bmatrix}\n    1 & 2 & -1\n    \\end{bmatrix}^T$:\n    \\begin{itemize}\n        \\item Obtenga los datos muéstrales (o puntuaciones) asociados a la variable $Y$.\n        \\item Realice la verificación de normalidad uni-variada de los datos asociados a $Y$.\n    \\end{itemize}\n\n  \\item Considere el vector definido por $\\mathbf{y} = \\mathbf{A} \\mathbf{x}^{(1)}$, con $\\mathbf{A} = \n    \\begin{bmatrix}\n    0 & 1 & 2 \\\\\n    2 & 0 & -1\n    \\end{bmatrix}$:\n    \\begin{itemize}\n        \\item Obtenga los datos muestrales (o puntuaciones) asociados al vector $\\mathbf{y}$.\n        \\item Realice la verificación de normalidad bi-variada de los datos asociados a $\\mathbf{y}$.\n    \\end{itemize}\n\\end{enumerate}\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\\newpage\n\n## Solución Punto-01\n\n\n### Solución (a)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatos_01_y_02 <- read.table(\"datos_puntos_01_02.txt\", header = T)\n\n# Separar los conjuntos de datos\n\nx1 <- as.matrix(datos_01_y_02[, 1:3])  # Variables V1, V2, V3\nx2 <- as.matrix(datos_01_y_02[, 4:6])  # Variables V4, V5, V6\n```\n:::\n\n\n\n\n\nPrimeramente verificaremos la normalidad uni-variada por medio de la prueba de Shapiro-Wilk a los datos asociados a $X^{(1)}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normalidad uni-variada\napply(x1, 2, function(col) shapiro.test(col))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$V1\n\n\tShapiro-Wilk normality test\n\ndata:  col\nW = 0.99036, p-value = 0.7069\n\n\n$V2\n\n\tShapiro-Wilk normality test\n\ndata:  col\nW = 0.9904, p-value = 0.7094\n\n\n$V3\n\n\tShapiro-Wilk normality test\n\ndata:  col\nW = 0.99186, p-value = 0.8206\n```\n\n\n:::\n:::\n\n\n\n\nLas pruebas de normalidad uni-variada para las variables $V_1$ , $V_2$ y $V_3$ no muestran evidencia suficiente para rechazar la hipótesis de normalidad. Por lo tanto, estas variables son consistentes con una distribución normal.\n\n\\newpage\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normalidad bi-variada\nbiv_pairs <- combn(1:3, 2, simplify = FALSE)\nfor (pair in biv_pairs) {\n  cat(sprintf(\"Variables: V%d y V%d\\n\", pair[1], pair[2]))\n  print(mshapiro.test(t(x1[, pair])))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariables: V1 y V2\n\n\tShapiro-Wilk normality test\n\ndata:  Z\nW = 0.98749, p-value = 0.4862\n\nVariables: V1 y V3\n\n\tShapiro-Wilk normality test\n\ndata:  Z\nW = 0.9933, p-value = 0.9113\n\nVariables: V2 y V3\n\n\tShapiro-Wilk normality test\n\ndata:  Z\nW = 0.98521, p-value = 0.3424\n```\n\n\n:::\n:::\n\n\n\n\nPara los pares de variables analizados $(V1-V2, V1-V3, V2-V3)$, no hay\nevidencia suficiente para rechazar la hipótesis de normalidad. Por lo tanto, se\nconsidera que las combinaciones bi-variadas de $x(1)$ son consistentes con\nuna distribución normal.\n\n\n\n\nAhora verificaremos verificaremos la normalidad 3-variada de los datos asociados a $X^{(1)}$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normalidad 3-variada con el test de Mardia\nMardia_x1 <- mvn(x1, mvnTest = \"mardia\")\nMardia_x1$multivariateNormality\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Test          Statistic           p value Result\n1 Mardia Skewness   9.28226670724539 0.505541420304724    YES\n2 Mardia Kurtosis -0.406880365335198 0.684095857551414    YES\n3             MVN               <NA>              <NA>    YES\n```\n\n\n:::\n:::\n\n\n\n\n- Para la asimetría, el p-valor alto (0.5055) indica que no hay evidencia suficiente para\nrechazar la normalidad.\n\n- Para la curtosis, el p-valor alto (0.6841) también sugiere que los datos cumplen con la normalidad.\n\n- La conclusión general del test (MVN) confirma que los datos en $x(1)$ son consistentes con una distribución normal multivariada.\n\n\n### Solución (b)\n\nLos estimadores de maxima verosimilitud para\n\n\n$$\n\\hat{\\mu} = \\underline{\\bar{\\mathbf{x}}}^{(1)} = \n\\begin{bmatrix}\n\\overline{X}_1 \\\\\n\\overline{X}_2 \\\\\n\\overline{X}_3\n\\end{bmatrix} = \n\\begin{bmatrix}\n0.06616429 \\\\\n-0.16939592 \\\\\n-0.34023163 \n\\end{bmatrix} \\ y \\quad\n$$\n\n$$\n\\hat{\\Sigma}_{11} = S_n = \\frac{1}{n} \\sum_{i=1}^n \\left(\\underline{\\mathbf{x}}^{(1)} - \\underline{\\bar{\\mathbf{x}}}^{(1)}\\right) \\left(\\underline{\\mathbf{x}}^{(1)} - \\underline{\\bar{\\mathbf{x}}}^{(1)}\\right)' = \\begin{bmatrix}\n4.369791 & -0.4407255 & 2.435705 \\\\\n-0.4407255 & 9.9162583 & -1.465279 \\\\\n2.4357049 & -1.4652786 & 5.628944\n\\end{bmatrix}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimador de la media muestral (Máxima Verosimilitud de µ(1))\nmu_1_hat <- colMeans(x1)\nprint(mu_1_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         V1          V2          V3 \n 0.06616429 -0.16939592 -0.34023163 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimador de la matriz de covarianza muestral (Máxima Verosimilitud de Σ11)\nsigma_11_hat <- cov(x1)\nprint(sigma_11_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           V1         V2        V3\nV1  4.3697911 -0.4407255  2.435705\nV2 -0.4407255  9.9162583 -1.465279\nV3  2.4357049 -1.4652786  5.628944\n```\n\n\n:::\n:::\n\n\n\n\n\n### Solución (c)\n\n\nSe nos pide calcular los valores de la variable $Y$, definida por:\n\n$$\n\\mathbf{Y} = \\mathbf{a}^T \\mathbf{x}^{(2)}, \\quad con \\quad\n\\mathbf{a} = \n    \\begin{bmatrix}\n    1 & 2 & -1\n    \\end{bmatrix}^T\\quad y \\quad\n\\underline{\\mathbf{x}}^{(2)} = \n\\begin{bmatrix}\nX_4 \\\\\nX_5 \\\\\nX_6\n\\end{bmatrix}    \n$$    \n\nHaciendo el calculo tenemos lo siguiente\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx2_data <- as.matrix(datos_01_y_02[, c(\"V4\", \"V5\", \"V6\")])\n\n# vector a\na <- c(1, 2, -1)\n\n# Calcular los valores de Y\ny_values <- x2_data %*% a\n\ny_values[1:10,] # Primeras 10 obervaciones de los valores de Y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] -4.2159  3.4235 -9.7821  2.9079  1.9770 -1.1429  6.1409 -2.8938  2.6106\n[10] -1.8939\n```\n\n\n:::\n:::\n\n\n\n\nAhora verifiquemos  la normalidad univariada a los valores asociados a $Y$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verificación de normalidad univariada con la prueba de Shapiro-Wilk\nshapiro.test(y_values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  y_values\nW = 0.99259, p-value = 0.8698\n```\n\n\n:::\n:::\n\n\nDado que el p-valor es 0.8698 que es significativamente mayor a 0.05, podriamos decir que los datos asociados a $Y$ podrían provenir de una distribución normal, lo cual es consistente con el supuesto de normalidad.\n\n\n### Solución (d)\n\n\n\nSe nos pide calcular los valores de la variable $Y$, definida por:\n\n\n$$\n\\mathbf{Y} = \\mathbf{A} \\underline{\\mathbf{x}}^{(1)}, \\quad con \\quad\n\\mathbf{A} = \n    \\begin{bmatrix}\n    0 & 1 & 2 \\\\\n    2 & 0 & -1\n    \\end{bmatrix} \\quad y \\quad\n\\underline{\\mathbf{x}}^{(1)} = \n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{bmatrix} \n$$\n\n\nHaciendo el calculo tenemos que\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Seleccionar las columnas correspondientes a x(1) \nx1_data <- as.matrix(datos_01_y_02[, c(\"V1\", \"V2\", \"V3\")])\n\n# Definir la matriz A\nA <- matrix(c(0, 1, 2,2, 0, -1), \n            nrow = 2, byrow = TRUE)\n\n# Calcular los valores de Y = A * x(1)\ny_values_d <- x1_data %*% t(A)\n\n\n# Primeras 10 filas de los valores de Y\ny_values_d[1:10, ] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]    [,2]\n [1,] -4.2824 -0.1716\n [2,] -0.5055  1.0423\n [3,] -4.0639  5.8175\n [4,]  2.8040  4.4765\n [5,] -5.4089  3.7831\n [6,] -2.9337 -1.8006\n [7,]  3.4953  1.3161\n [8,] -2.9879 -0.8263\n [9,]  2.9011 -0.8607\n[10,]  3.7672  1.2241\n```\n\n\n:::\n:::\n\n\n\n\nAhora verifiquemos  la normalidad-bivariada a los valores asociados a $Y$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Aplicar prueba de Mardia para normalidad bivariada\nmardia_test_d <- mvn(data = as.data.frame(y_values_d), \n                   mvnTest = \"mardia\")\n\nmardia_test_d$multivariateNormality\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Test         Statistic          p value Result\n1 Mardia Skewness   4.0057558363536 0.40522744241763    YES\n2 Mardia Kurtosis 0.143080994084797 0.88622621501471    YES\n3             MVN              <NA>             <NA>    YES\n```\n\n\n:::\n:::\n\n\n\n\n\n- Para la asimetría, el p-valor alto (0.4052) indica que no hay evidencia suficiente para\nrechazar la normalidad.\n\n- Para la curtosis, el p-valor alto (0.8862) también sugiere que los datos cumplen con la normalidad.\n\n- La conclusión general del test (MVN) confirma que los datos asociados a $Y$ son consistentes con una distribución normal multivariada.\n\n\n\n\n\n\n\n<!-- Punto 02 -->\n\n\n# Punto-02:\n\nA partir de los dos conjuntos de datos asociados a $\\mathbf{x}^{(1)}$ y $\\mathbf{x}^{(2)}$, realice los siguientes puntos:\n\n\\begin{enumerate}[label=(\\alph*)]\n    \\item Hallar $\\boldsymbol{\\mu}_{1 \\vert 2} = \\mathbb{E}[\\mathbf{x}^{(1)} \\mid \\mathbf{x}^{(2)}]$.\n    \n  \\item A partir de (a), ¿cuál es la matriz de coeficientes que resulta del ajuste de un Modelo de Regresión Lineal Multivariado (MRL-Multivariado) de $\\mathbf{x}^{(1)}$ versus $\\mathbf{x}^{(2)}$?\n    \n  \\item Utilizando teoría de modelos lineales, ajuste el MRL-Multivariado de $\\mathbf{x}^{(1)}$ versus $\\mathbf{x}^{(2)}$. Compare los coeficientes de dicho modelo ajustado con los obtenidos en (b).\n\\end{enumerate}\n\n\n\n\n\n## Solución Punto-02:\n\n\n### Solución (a)\n\n\n\nSe nos pide hallar $\\boldsymbol{\\mu}_{1 \\vert 2} = E[\\underline{\\mathbf{x}}^{(1)} \\mid \\underline{\\mathbf{x}}^{(2)}]$\n\n\nSabemos que\n\n$$\n\\hat{\\mu}_{1 \\mid 2} = \\underline{\\hat{\\mu}}^{(1)} + \\hat{\\Sigma}_{12} \\hat{\\Sigma}_{22}^{-1} \\left(\\underline{\\mathbf{x}}^{(2)} - \\underline{\\hat{\\mu}}^{(2)}\\right) \n$$\nHaciendo el calculo tenemos que\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Separar los conjuntos de datos\nx1 <- as.matrix(datos_01_y_02[, 1:3])  # Variables V1, V2, V3\nx2 <- as.matrix(datos_01_y_02[, 4:6])  # Variables V4, V5, V6\n\n# Calcular las medias muestrales\nmu1 <- colMeans(x1)\nmu2 <- colMeans(x2)\n\n# Calcular matrices de covarianza\nS11 <- cov(x1)\nS22 <- cov(x2)\nS12 <- cov(x1, x2)\nS21 <- t(S12)\n\n# Cálculo de la media condicional: μ_{1|2} = E[x(1) | x(2)]\nmu_1_given_2 <- function(x2_val) {\n  mu1 + S12 %*% solve(S22) %*% (x2_val - mu2)\n}\n\n# Aplicar la función a los valores observados de x(2)\nest_mu_1_given_2 <- t(apply(x2, 1, mu_1_given_2))\n\n# Primeras 10 observaciones\nest_mu_1_given_2 %>% head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]        [,2]        [,3]\n [1,] -1.3553765  4.37016752 -2.03552612\n [2,]  0.4128138 -0.84645970 -0.11007908\n [3,] -0.8155162  0.03930673 -2.16221587\n [4,]  0.9067611 -0.86654084  2.20719574\n [5,] -1.3863161  5.14597979 -2.78992467\n [6,]  0.3771059 -0.94286047  0.58990915\n [7,]  0.5327162  0.47273639  0.99986015\n [8,] -0.4323892  0.56521761 -1.40687991\n [9,]  0.8358329 -1.09426221  1.75353399\n[10,]  0.1331770 -0.63346006 -0.08993782\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Solucion (b)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Matriz de coeficientes del modelo de regresión\nB_hat <- S12 %*% solve(S22)\nprint(B_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             V4          V5          V6\nV1 -0.058743490  0.25345173 -0.07353095\nV2  0.553898748 -0.07750982  0.19987111\nV3  0.003132685  0.66344081  0.04630720\n```\n\n\n:::\n:::\n\n\n\n- Las filas de la matriz \\(B\\) representan las variables dependientes (\\(V1, V2, V3\\)).\n- Las columnas de la matriz \\(B\\) representan las variables independientes (\\(V4, V5, V6\\)).\n\nLos valores en la matriz \\(B\\) indican cómo cada variable independiente (\\(V4, V5, V6\\)) afecta, en promedio, a cada variable dependiente (\\(V1, V2, V3\\)).\n\nPor ejemplo:\n\n- El coeficiente $B_{(1, 4)} = -0.0587$ indica que un aumento de una unidad en \\(V4\\) disminuye \\(V1\\) en \\(0.0587\\) unidades, manteniendo constantes \\(V5\\) y \\(V6\\).\n\nEl coeficiente $B_{(2, 5)} = -0.0775$ indica que un aumento de una unidad en \\(V5\\) disminuye \\(V2\\) en \\(0.0775\\) unidades, manteniendo constantes \\(V4\\) y \\(V6\\).\n\n- El coeficiente $B_{(3, 5)} = 0.6634$ muestra que \\(V3\\) aumenta considerablemente cuando \\(V5\\) incrementa en una unidad.\n\n\n### Solución (c)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ajustar el modelo de regresión lineal multivariado\nmodelo <- lm(cbind(V1, V2, V3) ~ V4 + V5 + V6, data = datos_01_y_02)\n\n# Coeficientes obtenidos del modelo ajustado\ncoef_modelo <- coef(modelo)\n\nprint(coef_modelo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     V1          V2           V3\n(Intercept)  0.16445841 -0.50094574 -0.226650667\nV4          -0.05874349  0.55389875  0.003132685\nV5           0.25345173 -0.07750982  0.663440812\nV6          -0.07353095  0.19987111  0.046307196\n```\n\n\n:::\n:::\n\n\n\n\n\n\n- La matriz \\(B\\) calculada coincide exactamente con los coeficientes del modelo ajustado.\n\n- Esto valida que los cálculos teóricos y el modelo ajustado son consistentes.\n\n\nConclusión general\n\nLa matriz \\(B\\) y los coeficientes del modelo ajustado confirman que las relaciones entre las variables independientes (\\(V4, V5, V6\\)) y las dependientes (\\(V1, V2, V3\\)) son adecuadamente modeladas por un modelo de regresión lineal multivariado.\n \n\n\n<!-- Punto 03 -->\n\n\n# Punto-03:\n\n\nPara este punto, considere los dos conjuntos de datos asignados, los cuales corresponden a datos simulados de los vectores normales 3-variados independientes $\\mathbf{x}_1$ y $\\mathbf{x}_2$, con vector de medias y matriz de varianza-covarianza dados por:\n\n\n$$\n\\boldsymbol{\\mu} = \n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}, \\quad\n\\boldsymbol{\\Sigma} = \n\\begin{bmatrix}\n4 & 2 & 0 \\\\\n2 & 3 & 1 \\\\\n0 & 1 & 5\n\\end{bmatrix}\n$$\n\n\nEs decir, los dos conjuntos de datos son simulaciones de los vectores:\n\n$$\n\\mathbf{x}_1 \\sim N_3(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}), \\quad \\mathbf{x}_2 \\sim N_3(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}), \\quad \\text{Cov}(\\mathbf{x}_1, \\mathbf{x}_2) = \\mathbf{O}_{3 \\times 3}\n$$\n\nConsidere las siguientes combinaciones lineales de $\\mathbf{x}_1$ y $\\mathbf{x}_2$:\n\n\n$$\n\\mathbf{v}_1 = \\mathbf{x}_1 + 2 \\mathbf{x}_2, \\quad \\mathbf{v}_2 = 2 \\mathbf{x}_1 - \\mathbf{x}_2\n$$\n\n\n\n\\begin{enumerate}[label=(\\alph*)]\n    \\item Obtenga los datos muéstrales (o puntuaciones) asociados a los vectores $\\mathbf{v}_1$ y $\\mathbf{v}_2$.\n\n  \\item Realice la verificación de normalidad 3-variada de los datos asociados a $\\mathbf{v}_1$. \\\\\n    \\textbf{Nota:} Utilizar algunas de las herramientas vistas en clase sobre procesos de evaluación de la normalidad multivariada y/o herramientas que usted conozca para dichos procesos.\n\n  \\item Realice la verificación de normalidad 6-variada de los datos asociados al vector:\n\n\n$$\n\\mathbf{v} = \n\\begin{bmatrix}\n\\mathbf{v}_1 \\\\\n\\mathbf{v}_2\n\\end{bmatrix}\n$$\n\n\\textbf{Nota:} Utilizar algunas de las herramientas vistas en clase sobre procesos de evaluación de la normalidad multivariada y/o herramientas que usted conozca para dichos procesos.\n\n\\end{enumerate}\n\n\n\n\n\n## Solución: Punto-03\n\n\n### Solución (a)\n\n\nTenemos que \n\n\n$$\n\\mathbf{v}_1 = \\mathbf{x}_1 + 2 \\mathbf{x}_2, \\quad y \\quad \\mathbf{v}_2 = 2 \\mathbf{x}_1 - \\mathbf{x}_2\n$$\n\nHaciendo los calculos tenemos que\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cargando la muestras de datos de X(1) y X(2)\nx1_data_03 <- read.table(\"equipo_07_muestra1_datos_03.txt\")\nx2_data_03 <- read.table(\"equipo_07_muestra2_datos_03.txt\")\n\n\n# Calculando a V1 y V2\nv1 <- x1_data_03 + 2*x2_data_03\nv2 <- 2*x1_data_03 - x2_data_03\n```\n:::\n\n\n\n\nAhora vizualicemos las primeras 10 observaciones asociadas a $\\mathbf{v}_1$ y $\\mathbf{v}_2$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nv1 %>% head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        V1      V2       V3\n1  -1.8490 -2.5414 -13.9794\n2  -2.6448 -1.0773   1.9142\n3   0.4960 -2.9083  -6.0066\n4   1.5260  1.3293   0.3419\n5  -3.0829  0.9189  -0.3715\n6   1.8622  0.3212  -8.8208\n7   7.2936  7.2650  -3.2885\n8   3.2889  9.1205  -2.4975\n9   1.4521 -1.5028   5.1043\n10  1.5850  0.1867   3.9368\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nv2 %>% head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        V1      V2       V3\n1  -2.7475 -3.5153   0.2252\n2  14.1824  9.5144  -3.4876\n3   4.6625 -2.0701   8.4588\n4   7.1300 -0.5934   2.2308\n5  -2.0778 -3.4112  -3.0700\n6   0.6509  0.0249  -9.7021\n7  -1.9483 -5.7180   4.2535\n8   1.5703 -2.2480 -14.4340\n9   4.3702  2.0014  -1.5494\n10  4.0490 -1.3071   0.9296\n```\n\n\n:::\n:::\n\n\n\n\n### Solución (b)\n\nVerifiquemos la normalidad 3-variada de los datos asociados $\\mathbf{v}_1$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Aplicar prueba de Mardia para normalidad 3-variada\nmardia_test_v1 <- mvn(data = as.data.frame(v1), \n                   mvnTest = \"mardia\")\n\nmardia_test_v1$multivariateNormality\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Test        Statistic           p value Result\n1 Mardia Skewness 10.2906401172815 0.415375186124852    YES\n2 Mardia Kurtosis 1.09536098434238  0.27335851982769    YES\n3             MVN             <NA>              <NA>    YES\n```\n\n\n:::\n:::\n\n\n\n\n- Para la asimetría, el p-valor alto (0.4153) indica que no hay evidencia suficiente para rechazar la normalidad.\n\n- Para la curtosis, el p-valor alto (0.2733) también sugiere que los datos cumplen con la normalidad.\n\n- La conclusión general del test (MVN) confirma que los datos asociados a $\\mathbf{v}_1$ son consistentes con una distribución normal multivariada.\n\n\n\n\n\n### Solución (c)\n\n\nVeamos primeramente si $\\underline{\\mathbf{x}}_1$ y $\\underline{\\mathbf{x}}_2$ cumplen el supuesto de nomralidad 3-variada.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmardia_test_x1 <- mvn(data = as.data.frame(x1_data_03), mvnTest = \"mardia\")\nmardia_test_x1$multivariateNormality\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Test         Statistic            p value Result\n1 Mardia Skewness   20.595282974089 0.0240992905964353     NO\n2 Mardia Kurtosis 0.992560426477898  0.320924218144901    YES\n3             MVN              <NA>               <NA>     NO\n```\n\n\n:::\n:::\n\n\n\n\n- Para la asimetría, el p-valor bajo (0.0.0240) indica que si hay evidencia suficiente para rechazar la normalidad.\n\n- Para la curtosis, el p-valor alto (0.3209) también sugiere que los datos cumplen con la normalidad.\n\n- La conclusión general del test (MVN) confirma que los datos asociados a $\\underline{\\mathbf{x}}_1$ no son consistentes con una distribución normal multivariada.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmardia_test_x2 <- mvn(data = as.data.frame(x2_data_03), mvnTest = \"mardia\")\nmardia_test_x2$multivariateNormality\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Test        Statistic           p value Result\n1 Mardia Skewness 15.1833963323184  0.12551724045642    YES\n2 Mardia Kurtosis 1.18675599467803 0.235323881097516    YES\n3             MVN             <NA>              <NA>    YES\n```\n\n\n:::\n:::\n\n\n\nCon base en los resultados de la prueba de Mardia,no hay evidencia suficiente para rechazar la hipótesis de normalidad 3-variada. Por lo tanto, los datos asociados a $\\mathbf{x}_2$ se ajustan a una distribución normal 3-variada.\n\n\nDado que los datos asociados a $\\mathbf{x}_1$ no se distribuyen normal 3-variando, por tanto los datos asociados al vector $\\mathbf{v}$ que se compone de combinaciones lineales de $\\mathbf{x}_1$ y $\\mathbf{x}_2$ no cumpliran el supuesto de normalidad 6-variada.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}